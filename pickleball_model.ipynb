{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DIdsUlyLxJy0",
        "outputId": "d8070a03-f475-4261-ba19-03ddeec89749",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.2.72-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.18.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.4)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.7.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Downloading ultralytics-8.2.72-py3-none-any.whl (863 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m863.7/863.7 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading ultralytics_thop-2.0.0-py3-none-any.whl (25 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from collections import deque\n",
        "from ultralytics import YOLO\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NBW2vyTPeJSt",
        "outputId": "40b2ad92-89d6-44d9-a38a-755e597dd8ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'ultralytics'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3e962742acd0>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0multralytics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ultralytics'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO('yolov8n-pose.pt')\n",
        "swing_detect_model = load_model('/content/drive/MyDrive/swing_detect.h5')\n",
        "#shot_class_model =load_model('/content/drive/MyDrive/swing_class_100frames.h5')"
      ],
      "metadata": {
        "id": "SNjwa2ROfy9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "swing_detect_class=['swing_begin','swing_middle','swing_end']\n",
        "shot_class = ['forehand_stroke', 'forehand_slice', 'forehand_volley', 'backhand_stroke', 'backhand_volley', 'backhand_slice']"
      ],
      "metadata": {
        "id": "CZ9rQ1lAeT4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_frame(frame):\n",
        "\tframe = cv2.resize(frame, (640, 640))\n",
        "\tframe = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\tframe = frame.astype(np.float32) / 255.0\n",
        "\tframe = np.expand_dims(frame, axis=0)\n",
        "\tframe = np.transpose(frame, (0, 3, 1, 2))\n",
        "\treturn frame\n",
        "\n",
        "def extract_keypoints(frames):\n",
        "\tall_keypoints = []\n",
        "\tdevice = torch.device('cuda')\n",
        "\t# annotated_frames = []\n",
        "\tfor frame in frames:\n",
        "\t\tframe = preprocess_frame(frame)\n",
        "\t\tframe = torch.tensor(frame, dtype=torch.float32).to(device)\n",
        "\t\tresults = model(frame, verbose=False)\n",
        "\t\t# annotated_frame = results[0].plot()\n",
        "\n",
        "\t\tif len(results) > 0 and len(results[0].keypoints) > 0:\n",
        "\t\t\tkeypoints_list = results[0].keypoints\n",
        "\t\t\tbboxes = results[0].boxes\n",
        "\t\t\tif bboxes is not None and len(bboxes) > 0:\n",
        "\t\t\t\ttry:\n",
        "\t\t\t\t\txyxy = bboxes.xyxy.cpu().numpy()  # get xyxy coordinates to find the closest person to the camera.\n",
        "\t\t\t\t\tareas = [(box[2] - box[0]) * (box[3] - box[1]) for box in xyxy]\n",
        "\t\t\t\texcept IndexError as e:\n",
        "\t\t\t\t\tprint(f\"Error calculating areas: {e}\")\n",
        "\t\t\t\t\tprint(f\"Bounding boxes: {bboxes}\")\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\tmax_area_index = np.argmax(areas)  # get the index that contains the biggest bbox\n",
        "\t\t\t\tkeypoints = keypoints_list[max_area_index].xy[0].cpu().numpy()  # get the keypoints from the biggest bbox\n",
        "\t\t\t\tall_keypoints.append(keypoints)\n",
        "\t\t# out.write(annotated_frame)  # これswingの結果も書く必要があるので後に持っていく.\n",
        "\treturn all_keypoints, frames  # ここで次にannotated_framesを次に繋げる\n",
        "\n",
        "def normalize_keypoints(keypoints):\n",
        "\thip_index = 11  # index of left hip\n",
        "\tshoulder_index = 5  # index of shoulder\n",
        "\n",
        "\tnormalized_keypoints = []\n",
        "\tfor frame_keypoints in keypoints:\n",
        "\t\tif len(frame_keypoints) > max(hip_index, shoulder_index):\n",
        "\t\t\thip_point = frame_keypoints[hip_index]\n",
        "\t\t\tshoulder_point = frame_keypoints[shoulder_index]\n",
        "\t\t\t# set hip point as (0,0) coordinates, set distance from hip to shoulder as 1 in every input.\n",
        "\t\t\trelative_points = frame_keypoints - hip_point\n",
        "\t\t\t# scaling\n",
        "\t\t\tscale_factor = np.linalg.norm(shoulder_point - hip_point)\n",
        "\t\t\tif scale_factor != 0:\n",
        "\t\t\t\trelative_points /= scale_factor\n",
        "\t\t\tnormalized_keypoints.append(relative_points)\n",
        "\t\telse:\n",
        "\t\t\tprint(f\"Warning: Frame with insufficient keypoints detected. Skipping this frame.\")\n",
        "\n",
        "\treturn np.array(normalized_keypoints)\n"
      ],
      "metadata": {
        "id": "IVazGlaIjTuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rescale_keypoints(keypoints, input_size, output_size):\n",
        "  scale_x = output_size[0] / input_size[0]\n",
        "  scale_y = output_size[1] / input_size[1]\n",
        "  rescaled_keypoints = []\n",
        "  for kp in keypoints:\n",
        "    rescaled_keypoints.append([kp[0] * scale_x, kp[1] * scale_y])\n",
        "  return rescaled_keypoints\n",
        "\n",
        "def draw_keypoints(frame, original_keypoints, color=(0, 255, 0), radius=5):\n",
        "\tfor kp in original_keypoints:\n",
        "\t\tif kp is not None and len(kp) == 2:\n",
        "\t\t\tx, y = int(kp[0]), int(kp[1])\n",
        "\t\t\tcv2.circle(frame, (x, y), radius, color, -1)\n",
        "\treturn frame\n",
        "\n",
        "def draw_rec(all_keypoints,frames):\n",
        "\tannotated_frames = []\n",
        "\tfor point in all_keypoints:\n",
        "\t\tx, y = point[:2]\n",
        "\t\tcv2.circle(annotated_frame, (int(x), int(y)), 5, (0, 255, 0), -1)\n",
        "\t\tannotated_frames.append(annotated_frame)\n",
        "\treturn\n",
        "\n",
        "def add_text (new_frame):\n",
        "  texted_frame = cv2.putText(new_frame, \"Swing\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "  return texted_frame\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AB9grJ0VU9ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGenerator(Sequence):\n",
        "\tdef __init__(self, x_set, batch_size):\n",
        "\t\tself.x = x_set\n",
        "\t\tself.batch_size = batch_size\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn int(np.ceil(len(self.x) / float(self.batch_size)))\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\tbatch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\t\treturn batch_x"
      ],
      "metadata": {
        "id": "awo_2N5_jTrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"/content/drive/MyDrive/test_30s.mp4\""
      ],
      "metadata": {
        "id": "P5U-X0s4jTFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow"
      ],
      "metadata": {
        "id": "Yqfi1Z4YCYJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "frame_buffer = deque(maxlen=40)\n",
        "is_recording_swing = False\n",
        "swing_record = []\n",
        "\n",
        "# VideoWriterの設定\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # または適切なコーデック\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "width, height = 640, 640\n",
        "original_width, original_height = 1280,720\n",
        "out = cv2.VideoWriter('output.mp4', fourcc, fps, (original_width, original_height))\n",
        "\n",
        "if not out.isOpened():\n",
        "\tprint(\"Error: VideoWriter not opened.\")\n",
        "\tcap.release()\n",
        "\tcv2.destroyAllWindows()\n",
        "\texit()\n",
        "\n",
        "while cap.isOpened():\n",
        "\tret, frame = cap.read()\n",
        "\tif not ret:\n",
        "\t\tbreak\n",
        "\n",
        "\tframe_buffer.append(frame)\n",
        "  swing_record = []\n",
        "\n",
        "  if (len(frame_buffer) < 40):\n",
        "    new_frame = frame_bufer[-1]\n",
        "    keypoints, frames = extract_keypoints(new_frame)\n",
        "    original_keypoints = rescale_keypoints(keypoints, (640,640), (1820,720))\n",
        "    new_frame = draw_keypoints(new_frame, original_keypoints, color=(0, 255, 0), radius=5)\n",
        "    out.write(new_frame)\n",
        "\n",
        "\telif (len(frame_buffer) == 40):\n",
        "\t\tkeypoints, frames = extract_keypoints(frame_buffer)\n",
        "\t\tif len(keypoints) > 0:\n",
        "\t\t\tnormalized_keypoints = normalize_keypoints(keypoints)\n",
        "\t\t\tX_array = np.array(normalized_keypoints)\n",
        "\t\t\tX_array = X_array.reshape((1, X_array.shape[0], X_array.shape[1] * X_array.shape[2]))\n",
        "\t\t\tgenerator = DataGenerator(X_array, batch_size=1)\n",
        "\t\t\tpredictions = swing_detect_model.predict(generator)\n",
        "\t\t\tswing_prediction = np.argmax(predictions, axis=1)\n",
        "\t\t\tif swing_prediction[0] != 2:   #スイング最中\n",
        "\t\t\t\tis_recording_swing = True\n",
        "\t\t\t\tprint(\"swinging\")\n",
        "        swing_record.extend([cv2.resize(new_frame, (original_width, original_height)) for new_frame in list(frames)[-3:-1]])\n",
        "\n",
        "      elif swing_prediction[0] == 2:  # not swinging\n",
        "        if is_recording_swing:  #the end of swing\n",
        "          new_frames=[]\n",
        "          is_recording_swing = False\n",
        "          for new_frame in swing_record:\n",
        "            new_frame =cv2.resize(new_frame, (1280, 720))\n",
        "            original_keypoints = rescale_keypoints(keypoints, (640,640), (1820,720))\n",
        "            print(original_keypoints[0])\n",
        "            print(original_keypoints[1])\n",
        "            print(original_keypoints[2])\n",
        "            print(original_keypoints[3])\n",
        "            new_frame = draw_keypoints(new_frame, original_keypoints, color=(0, 255, 0), radius=5)\n",
        "            new_frame = add_text(new_frame)\n",
        "            out.write(new_frame)\n",
        "          swing_record.clear()\n",
        "\n",
        "        else: #停止中。ここに来るたびvideoにframeを追加\n",
        "          end_swing = extend([cv2.resize(frame, (original_width, original_height)) for frame in list(frames)[-2:]])\n",
        "          for new_frame in end_swing:\n",
        "            original_keypoints = rescale_keypoints(keypoints[-2:], (640,640), (1820,720))\n",
        "            new_frame= draw_keypoints(new_frame, original_keypoints, color=(0, 255, 0), radius=5)\n",
        "            out.write(new_frame)\n",
        "\n",
        "\t\tfor _ in range(2):\n",
        "\t\t\tif frame_buffer:\n",
        "\t\t\t\tframe_buffer.popleft()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# リソースの解放\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n"
      ],
      "metadata": {
        "id": "-kvhb4aypevd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/output.mp4 /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "EcBtHgLfpe24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bNVPRF4fpe60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "B0YAMZ6xEZAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0JA7hxXOpe9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dOj94JippfBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "auT9Os9ZpfEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zcqpdT4Ipfgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z9i9CS-_pfkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_swing_start(keypoints_sequence):\n",
        "    if keypoints_sequence is not None and len(keypoints_sequence) == 35:\n",
        "        keypoints_sequence = np.array(keypoints_sequence)\n",
        "        keypoints_sequence = np.expand_dims(keypoints_sequence, axis=0)\n",
        "        prediction = swing_start_model.predict(keypoints_sequence)\n",
        "        return prediction[0][0] > 0.5  # 閾値は適宜調整してください\n",
        "    return False\n",
        "\n",
        "def classify_swing(keypoints_sequence):\n",
        "    if keypoints_sequence is not None and len(keypoints_sequence) == 30:\n",
        "        keypoints_sequence = np.array(keypoints_sequence)\n",
        "        keypoints_sequence = np.expand_dims(keypoints_sequence, axis=0)\n",
        "        prediction = swing_classification_model.predict(keypoints_sequence)\n",
        "        return np.argmax(prediction)\n",
        "    return None\n",
        "\n",
        "# 動画ファイルの読み込み\n",
        "cap = cv2.VideoCapture('path/to/your/video.mp4')\n",
        "\n",
        "frame_buffer = deque(maxlen=35)\n",
        "swing_frames = deque(maxlen=30)\n",
        "is_recording_swing = False\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    keypoints = extract_keypoints(frame)\n",
        "    normalized_keypoints = normalize_keypoints(keypoints)\n",
        "\n",
        "    if normalized_keypoints is not None:\n",
        "        frame_buffer.append(normalized_keypoints)\n",
        "\n",
        "        if len(frame_buffer) == 35:\n",
        "            if is_swing_start(list(frame_buffer)):\n",
        "                is_recording_swing = True\n",
        "                swing_frames.clear()\n",
        "                swing_frames.extend(list(frame_buffer)[-30:])  # 最後の30フレームを使用\n",
        "\n",
        "        if is_recording_swing:\n",
        "            swing_frames.append(normalized_keypoints)\n",
        "\n",
        "            if len(swing_frames) == 30:\n",
        "                swing_class = classify_swing(list(swing_frames))\n",
        "                if swing_class is not None:\n",
        "                    label = shot_types[swing_class]\n",
        "                    cv2.putText(frame, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "                is_recording_swing = False\n",
        "\n",
        "    cv2.imshow('Swing Classification', frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "    # 3フレームごとにframe_bufferの先頭を削除\n",
        "    if len(frame_buffer) == 35:\n",
        "        for _ in range(3):\n",
        "            if frame_buffer:\n",
        "                frame_buffer.popleft()\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "YRXMoOWZpfzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = []\n",
        "for label, shot_type in enumerate(shot_types):\n",
        "    folder_path = f'/Users/yusuke.s/Documents/pickleball_videos/{shot_type}'\n",
        "    all_data.extend(process_video(folder_path, label))"
      ],
      "metadata": {
        "id": "QxDOG1wbhO_s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}