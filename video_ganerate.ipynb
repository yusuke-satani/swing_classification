{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7012757-e741-4cf7-be4f-80cbe0bb7ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 352x640 1 person, 242.0ms\n",
      "Speed: 11.9ms preprocess, 242.0ms inference, 21.2ms postprocess per image at shape (1, 3, 352, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# モデルの読み込み。姿勢推論用のモデルデータを読み込む\n",
    "model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "pic = \"federer_backhand.jpg\"\n",
    "frame = cv2.imread(pic)\n",
    "result = model(frame)\n",
    "\n",
    "plottedFrame = result[0].plot()\n",
    "cv2.imwrite(pic + \"-result.jpg\", plottedFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d91bf82d-23a5-4b48-9227-5c829797b28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Roger Federer & Tomas Berdych ｜ IW Court Level Practice.mp4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Roger Federer & Tomas Berdych ｜ IW Court Level Practice.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "239d7b2a-973b-41c4-85a9-8cab0467e24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imageio in /Users/yusuke.s/opt/anaconda3/lib/python3.8/site-packages (2.9.0)\n",
      "Requirement already satisfied: numpy in /Users/yusuke.s/opt/anaconda3/lib/python3.8/site-packages (from imageio) (1.23.3)\n",
      "Requirement already satisfied: pillow in /Users/yusuke.s/opt/anaconda3/lib/python3.8/site-packages (from imageio) (8.0.0)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f679d149-1b01-40d6-a5a1-a80a6b64fc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f87094cb-4d13-4a25-aacf-28fd2216b815",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 371.8ms\n",
      "Speed: 10.8ms preprocess, 371.8ms inference, 30.1ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 0 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [558.8593, 223.6208],\n",
      "         [596.8178, 224.9405],\n",
      "         [525.6777, 269.9388],\n",
      "         [614.7839, 272.6838],\n",
      "         [506.8005, 335.6578],\n",
      "         [626.2600, 343.6624],\n",
      "         [531.6067, 362.1826],\n",
      "         [614.4081, 380.4658],\n",
      "         [536.7409, 398.5438],\n",
      "         [598.7604, 398.3880],\n",
      "         [540.2319, 510.3312],\n",
      "         [613.4751, 505.3246],\n",
      "         [535.3020, 613.5496],\n",
      "         [624.7803, 608.9522]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 213.4ms\n",
      "Speed: 2.4ms preprocess, 213.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 1 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [597.2088, 218.4986],\n",
      "         [526.5095, 266.0024],\n",
      "         [614.4534, 267.5851],\n",
      "         [507.5121, 333.5380],\n",
      "         [625.0742, 340.5450],\n",
      "         [530.0820, 357.2502],\n",
      "         [613.3513, 379.6679],\n",
      "         [537.9793, 393.7163],\n",
      "         [599.9675, 393.3109],\n",
      "         [534.9089, 505.1368],\n",
      "         [613.1331, 501.8865],\n",
      "         [527.1289, 607.6443],\n",
      "         [625.8493, 606.9992]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 197.0ms\n",
      "Speed: 1.2ms preprocess, 197.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 2 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [597.1799, 216.0552],\n",
      "         [525.6005, 263.9455],\n",
      "         [613.7997, 266.2774],\n",
      "         [508.1237, 331.5877],\n",
      "         [624.9610, 340.7706],\n",
      "         [533.0618, 354.9977],\n",
      "         [611.2580, 377.5485],\n",
      "         [534.8766, 390.2289],\n",
      "         [598.7971, 389.9990],\n",
      "         [526.7472, 504.4233],\n",
      "         [614.9606, 501.5836],\n",
      "         [513.1132, 605.6732],\n",
      "         [631.4586, 606.2900]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 4 persons, 346.5ms\n",
      "Speed: 1.7ms preprocess, 346.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 3 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [595.9763, 219.4681],\n",
      "         [527.2029, 267.4388],\n",
      "         [614.8669, 269.8438],\n",
      "         [510.9960, 335.2044],\n",
      "         [625.5920, 346.9616],\n",
      "         [531.8005, 350.9679],\n",
      "         [614.3562, 379.3503],\n",
      "         [536.4240, 395.9774],\n",
      "         [601.4370, 397.3541],\n",
      "         [520.2883, 506.6128],\n",
      "         [621.0608, 507.6626],\n",
      "         [501.8678, 608.0445],\n",
      "         [636.3643, 611.5117]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 197.1ms\n",
      "Speed: 2.3ms preprocess, 197.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 4 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [559.3168, 225.4072],\n",
      "         [594.9176, 224.2772],\n",
      "         [526.6285, 273.8632],\n",
      "         [614.4788, 275.7854],\n",
      "         [510.3305, 337.5932],\n",
      "         [628.4921, 354.3336],\n",
      "         [522.6691, 346.2310],\n",
      "         [617.8704, 383.7585],\n",
      "         [536.3976, 402.0992],\n",
      "         [602.9767, 402.9712],\n",
      "         [515.1441, 510.7137],\n",
      "         [623.8840, 511.9125],\n",
      "         [483.8523, 610.4890],\n",
      "         [636.8916, 615.1837]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 193.2ms\n",
      "Speed: 1.5ms preprocess, 193.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 5 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [557.0918, 234.1543],\n",
      "         [596.0844, 232.5272],\n",
      "         [526.6837, 284.2100],\n",
      "         [613.5999, 284.5464],\n",
      "         [511.5415, 351.9754],\n",
      "         [626.8131, 361.8705],\n",
      "         [534.3683, 360.2150],\n",
      "         [617.1817, 381.8439],\n",
      "         [533.1454, 413.3296],\n",
      "         [599.9872, 414.6761],\n",
      "         [505.9428, 512.6027],\n",
      "         [628.6835, 513.3020],\n",
      "         [473.7886, 613.6389],\n",
      "         [642.2052, 613.5020]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 186.9ms\n",
      "Speed: 1.5ms preprocess, 186.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 6 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [596.3979, 247.9539],\n",
      "         [525.7953, 298.8656],\n",
      "         [614.4549, 300.9717],\n",
      "         [509.6309, 366.3743],\n",
      "         [625.1514, 380.0817],\n",
      "         [537.0656, 382.1549],\n",
      "         [617.2492, 411.0949],\n",
      "         [532.6068, 428.3513],\n",
      "         [601.3332, 430.6813],\n",
      "         [503.1236, 518.0639],\n",
      "         [630.5614, 522.7110],\n",
      "         [465.3858, 618.2261],\n",
      "         [640.8496, 624.6304]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 188.0ms\n",
      "Speed: 1.4ms preprocess, 188.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 7 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [553.4946, 259.7813],\n",
      "         [594.4034, 261.6465],\n",
      "         [525.9958, 307.7981],\n",
      "         [611.6830, 316.7224],\n",
      "         [510.1389, 371.6468],\n",
      "         [626.4627, 394.8679],\n",
      "         [532.8951, 390.2928],\n",
      "         [616.2037, 413.6969],\n",
      "         [529.5567, 440.6980],\n",
      "         [598.0770, 443.9408],\n",
      "         [496.3948, 526.2800],\n",
      "         [622.8337, 529.2380],\n",
      "         [462.6382, 620.4417],\n",
      "         [640.0634, 624.5121]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 185.4ms\n",
      "Speed: 1.6ms preprocess, 185.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 8 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [590.8905, 273.6805],\n",
      "         [523.8698, 316.1975],\n",
      "         [605.4705, 323.7753],\n",
      "         [511.9865, 373.5128],\n",
      "         [622.5606, 399.3070],\n",
      "         [524.5291, 385.8061],\n",
      "         [606.3329, 419.1363],\n",
      "         [525.2496, 447.0885],\n",
      "         [590.2098, 450.7531],\n",
      "         [492.4873, 530.9909],\n",
      "         [622.0314, 532.7732],\n",
      "         [461.6974, 626.5352],\n",
      "         [645.2085, 624.2054]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 186.6ms\n",
      "Speed: 1.4ms preprocess, 186.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 9 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [588.1515, 276.7236],\n",
      "         [520.1119, 316.8118],\n",
      "         [600.2998, 325.0643],\n",
      "         [510.5053, 376.5972],\n",
      "         [615.7924, 402.1918],\n",
      "         [524.7045, 395.2206],\n",
      "         [598.1641, 427.0526],\n",
      "         [517.7798, 445.4443],\n",
      "         [582.0240, 449.6066],\n",
      "         [489.6246, 527.5879],\n",
      "         [617.9897, 530.3102],\n",
      "         [460.9333, 624.3183],\n",
      "         [643.0411, 623.4904]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 191.2ms\n",
      "Speed: 1.4ms preprocess, 191.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 10 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [587.9183, 273.2570],\n",
      "         [515.7534, 313.1854],\n",
      "         [598.7151, 324.8670],\n",
      "         [503.5082, 378.8806],\n",
      "         [613.7434, 403.5484],\n",
      "         [524.6787, 407.2198],\n",
      "         [595.8319, 425.4794],\n",
      "         [508.6459, 442.6105],\n",
      "         [576.4980, 449.1550],\n",
      "         [479.3063, 520.3390],\n",
      "         [614.4336, 529.1740],\n",
      "         [452.1353, 615.5138],\n",
      "         [645.1423, 625.0114]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 184.9ms\n",
      "Speed: 1.5ms preprocess, 184.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 11 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [582.2219, 269.1885],\n",
      "         [509.8241, 305.2731],\n",
      "         [586.3019, 319.7438],\n",
      "         [495.1420, 369.6434],\n",
      "         [592.8508, 396.4294],\n",
      "         [513.4125, 406.1461],\n",
      "         [588.9137, 420.5934],\n",
      "         [492.5019, 431.2693],\n",
      "         [555.0427, 439.1483],\n",
      "         [473.9689, 509.0808],\n",
      "         [599.4547, 522.2923],\n",
      "         [449.2853, 603.0046],\n",
      "         [631.1473, 620.8040]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 183.6ms\n",
      "Speed: 1.4ms preprocess, 183.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 12 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [579.5351, 263.2125],\n",
      "         [504.6924, 299.1304],\n",
      "         [578.4904, 315.0698],\n",
      "         [492.9673, 362.1955],\n",
      "         [581.3303, 392.9337],\n",
      "         [529.7020, 395.9497],\n",
      "         [595.8456, 414.8092],\n",
      "         [477.8239, 423.0280],\n",
      "         [539.1929, 430.9852],\n",
      "         [464.0581, 505.3389],\n",
      "         [589.9631, 517.5308],\n",
      "         [432.2145, 600.7136],\n",
      "         [616.2325, 618.3116]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 183.1ms\n",
      "Speed: 1.6ms preprocess, 183.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 13 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [570.2401, 263.3377],\n",
      "         [492.0622, 295.5974],\n",
      "         [565.9532, 313.2391],\n",
      "         [472.1093, 354.5952],\n",
      "         [569.3650, 391.3078],\n",
      "         [490.2105, 377.4651],\n",
      "         [593.1452, 412.9603],\n",
      "         [467.6495, 418.3099],\n",
      "         [528.9890, 426.5923],\n",
      "         [450.4931, 501.1868],\n",
      "         [581.5947, 514.7236],\n",
      "         [409.4139, 595.1010],\n",
      "         [600.2693, 616.3948]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 187.1ms\n",
      "Speed: 1.4ms preprocess, 187.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 14 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [556.3712, 264.0150],\n",
      "         [479.8126, 299.6167],\n",
      "         [552.0161, 315.0821],\n",
      "         [456.8564, 358.0805],\n",
      "         [559.1794, 389.4657],\n",
      "         [473.2903, 377.3306],\n",
      "         [599.1037, 409.7322],\n",
      "         [458.5247, 420.2925],\n",
      "         [517.5335, 427.5421],\n",
      "         [450.3623, 502.7008],\n",
      "         [574.8497, 517.5009],\n",
      "         [384.6978, 594.0646],\n",
      "         [562.2053, 619.6606]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 183.7ms\n",
      "Speed: 1.8ms preprocess, 183.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 15 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [544.1312, 268.1607],\n",
      "         [468.9883, 306.2563],\n",
      "         [541.1401, 317.2621],\n",
      "         [444.7967, 365.1562],\n",
      "         [549.6592, 388.4564],\n",
      "         [477.7798, 382.3196],\n",
      "         [598.3503, 410.0276],\n",
      "         [449.4608, 429.3350],\n",
      "         [510.1243, 434.1150],\n",
      "         [438.1139, 512.6677],\n",
      "         [571.4116, 521.1317],\n",
      "         [360.1235, 603.5255],\n",
      "         [558.6368, 621.7543]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 180.6ms\n",
      "Speed: 1.8ms preprocess, 180.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 16 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [532.9397, 276.1946],\n",
      "         [460.9691, 312.3906],\n",
      "         [527.6829, 325.3013],\n",
      "         [432.0094, 367.1144],\n",
      "         [542.8604, 397.0805],\n",
      "         [449.0541, 370.2611],\n",
      "         [596.7942, 406.6797],\n",
      "         [443.7242, 434.8136],\n",
      "         [499.8392, 440.4640],\n",
      "         [430.5619, 514.9453],\n",
      "         [562.8331, 524.2579],\n",
      "         [358.5956, 602.0034],\n",
      "         [558.4595, 621.4138]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 180.1ms\n",
      "Speed: 1.3ms preprocess, 180.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 17 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [523.7801, 281.9046],\n",
      "         [452.6836, 315.1208],\n",
      "         [514.1552, 330.7190],\n",
      "         [417.9880, 367.2476],\n",
      "         [529.1252, 400.2405],\n",
      "         [425.1028, 365.2452],\n",
      "         [572.3636, 399.9130],\n",
      "         [429.8725, 443.7070],\n",
      "         [482.0161, 451.9257],\n",
      "         [414.6834, 517.4896],\n",
      "         [552.4155, 527.9692],\n",
      "         [354.6012, 602.1790],\n",
      "         [560.2126, 617.8848]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 179.4ms\n",
      "Speed: 2.0ms preprocess, 179.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 18 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [510.5156, 285.5486],\n",
      "         [440.4547, 313.3034],\n",
      "         [497.1351, 333.3739],\n",
      "         [405.2497, 360.0758],\n",
      "         [505.0884, 400.9654],\n",
      "         [417.4371, 368.6340],\n",
      "         [541.4059, 406.5641],\n",
      "         [411.8960, 439.8383],\n",
      "         [461.6991, 449.3742],\n",
      "         [399.6916, 522.2445],\n",
      "         [533.9073, 531.3201],\n",
      "         [350.6208, 605.8872],\n",
      "         [557.6490, 617.9077]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 179.4ms\n",
      "Speed: 1.3ms preprocess, 179.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 19 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [496.6973, 285.3942],\n",
      "         [426.5984, 309.9755],\n",
      "         [476.4913, 331.9985],\n",
      "         [387.5329, 357.8049],\n",
      "         [479.6386, 408.1955],\n",
      "         [404.4683, 367.9502],\n",
      "         [518.1108, 414.8483],\n",
      "         [396.5715, 436.5245],\n",
      "         [440.7194, 445.4445],\n",
      "         [386.5406, 522.2268],\n",
      "         [505.1222, 527.0089],\n",
      "         [346.9733, 602.6623],\n",
      "         [540.2577, 610.8352]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 185.0ms\n",
      "Speed: 1.3ms preprocess, 185.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 20 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [481.2021, 292.7111],\n",
      "         [416.7767, 310.5107],\n",
      "         [461.5560, 334.9525],\n",
      "         [375.0828, 351.2689],\n",
      "         [459.9589, 409.7383],\n",
      "         [381.2506, 365.6379],\n",
      "         [484.2324, 421.3211],\n",
      "         [383.0712, 433.3523],\n",
      "         [422.0152, 443.3707],\n",
      "         [370.0643, 521.4781],\n",
      "         [483.4535, 525.9868],\n",
      "         [339.4664, 610.2841],\n",
      "         [516.0394, 610.8292]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 181.4ms\n",
      "Speed: 1.7ms preprocess, 181.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 21 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [480.2378, 290.5758],\n",
      "         [405.0836, 307.5487],\n",
      "         [459.4435, 331.2013],\n",
      "         [361.3453, 345.7571],\n",
      "         [438.2759, 395.0329],\n",
      "         [375.7861, 382.3896],\n",
      "         [437.5317, 415.5902],\n",
      "         [356.2884, 422.8760],\n",
      "         [401.4906, 429.2826],\n",
      "         [364.6099, 518.4742],\n",
      "         [466.4598, 518.2620],\n",
      "         [343.0456, 607.0599],\n",
      "         [496.9146, 607.7466]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 182.5ms\n",
      "Speed: 1.4ms preprocess, 182.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 22 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [470.7238, 286.6235],\n",
      "         [396.0559, 301.6049],\n",
      "         [452.7826, 326.4426],\n",
      "         [350.7421, 340.3563],\n",
      "         [428.7804, 391.4451],\n",
      "         [354.4484, 361.5131],\n",
      "         [408.2897, 390.6243],\n",
      "         [342.4631, 418.2307],\n",
      "         [387.4407, 425.1870],\n",
      "         [361.3226, 507.8148],\n",
      "         [453.0854, 512.0292],\n",
      "         [336.7115, 606.0226],\n",
      "         [468.6626, 614.2598]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 184.4ms\n",
      "Speed: 1.4ms preprocess, 184.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 23 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [458.3334, 284.8703],\n",
      "         [383.8544, 301.9989],\n",
      "         [447.4714, 321.7990],\n",
      "         [338.9598, 344.8315],\n",
      "         [432.5506, 386.3649],\n",
      "         [339.1469, 359.7228],\n",
      "         [408.3666, 383.7955],\n",
      "         [338.1688, 419.4996],\n",
      "         [384.8479, 426.9131],\n",
      "         [357.1851, 505.5146],\n",
      "         [434.8538, 512.2454],\n",
      "         [331.4603, 601.6774],\n",
      "         [429.7638, 610.5098]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 183.9ms\n",
      "Speed: 1.4ms preprocess, 183.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 24 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [455.5092, 279.9113],\n",
      "         [373.8780, 293.7569],\n",
      "         [451.0081, 321.6018],\n",
      "         [330.6373, 325.7414],\n",
      "         [426.2682, 385.1912],\n",
      "         [334.0365, 334.8410],\n",
      "         [408.4348, 387.5718],\n",
      "         [321.5215, 415.3542],\n",
      "         [374.7409, 424.7059],\n",
      "         [352.6831, 501.9871],\n",
      "         [416.7615, 511.2563],\n",
      "         [329.5690, 603.6967],\n",
      "         [392.4020, 610.2743]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 190.7ms\n",
      "Speed: 1.7ms preprocess, 190.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 25 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [447.9420, 277.2382],\n",
      "         [370.7284, 290.6652],\n",
      "         [434.0806, 318.9502],\n",
      "         [320.9612, 326.1055],\n",
      "         [412.8127, 382.0073],\n",
      "         [308.9521, 345.4425],\n",
      "         [388.0109, 387.3830],\n",
      "         [318.3203, 413.7010],\n",
      "         [360.8300, 423.5838],\n",
      "         [347.2902, 504.2713],\n",
      "         [393.4816, 512.4446],\n",
      "         [328.4352, 601.6177],\n",
      "         [361.3955, 603.4948]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 181.2ms\n",
      "Speed: 1.6ms preprocess, 181.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 26 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [440.3861, 270.0793],\n",
      "         [355.1457, 284.3900],\n",
      "         [431.3177, 313.2371],\n",
      "         [303.1230, 324.5619],\n",
      "         [402.4792, 382.9536],\n",
      "         [305.6660, 358.2551],\n",
      "         [382.8784, 398.7797],\n",
      "         [303.4887, 403.3872],\n",
      "         [352.3517, 411.1414],\n",
      "         [333.2492, 494.8599],\n",
      "         [370.7791, 509.6180],\n",
      "         [309.0761, 576.5045],\n",
      "         [320.6646, 596.5109]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 184.0ms\n",
      "Speed: 1.3ms preprocess, 184.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 27 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [385.5223, 260.6552],\n",
      "         [424.9305, 274.7713],\n",
      "         [343.5457, 283.5298],\n",
      "         [422.7165, 316.8710],\n",
      "         [294.4397, 322.8772],\n",
      "         [410.5363, 385.4435],\n",
      "         [284.4897, 385.7985],\n",
      "         [383.7938, 404.4957],\n",
      "         [295.4385, 406.9815],\n",
      "         [341.2576, 416.4065],\n",
      "         [334.1857, 508.7756],\n",
      "         [332.5340, 521.3815],\n",
      "         [320.9479, 594.8213],\n",
      "         [272.9323, 602.2760]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 183.0ms\n",
      "Speed: 1.4ms preprocess, 183.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 28 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [375.4786, 257.3637],\n",
      "         [423.8510, 269.8967],\n",
      "         [335.3710, 287.5520],\n",
      "         [422.3077, 314.1424],\n",
      "         [288.1841, 327.6971],\n",
      "         [406.3307, 374.4280],\n",
      "         [277.1922, 392.8658],\n",
      "         [376.4124, 405.1420],\n",
      "         [289.5997, 418.0047],\n",
      "         [339.2961, 426.0593],\n",
      "         [322.9303, 509.9288],\n",
      "         [314.5038, 522.3373],\n",
      "         [323.0631, 602.9951],\n",
      "         [242.7517, 607.3477]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 182.0ms\n",
      "Speed: 1.4ms preprocess, 182.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 29 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [366.4851, 258.1907],\n",
      "         [412.3091, 269.1805],\n",
      "         [326.2995, 287.8516],\n",
      "         [411.1176, 316.9215],\n",
      "         [279.2065, 327.0228],\n",
      "         [395.9849, 379.8793],\n",
      "         [270.7444, 390.7248],\n",
      "         [365.0179, 406.7765],\n",
      "         [285.6985, 417.5717],\n",
      "         [333.3286, 427.3209],\n",
      "         [313.5685, 504.5582],\n",
      "         [296.1320, 524.5646],\n",
      "         [318.0852, 594.2312],\n",
      "         [231.6353, 613.7339]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 770.6ms\n",
      "Speed: 1.3ms preprocess, 770.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 30 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [356.2354, 261.0184],\n",
      "         [396.8928, 270.5085],\n",
      "         [314.9823, 292.7170],\n",
      "         [397.9529, 319.3130],\n",
      "         [268.2061, 338.3372],\n",
      "         [394.4560, 386.4612],\n",
      "         [258.7271, 394.3972],\n",
      "         [364.9045, 403.9572],\n",
      "         [280.3664, 419.3413],\n",
      "         [324.9602, 427.9762],\n",
      "         [312.2371, 505.4233],\n",
      "         [282.5550, 521.4940],\n",
      "         [318.6546, 598.1960],\n",
      "         [222.7123, 613.6719]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 278.0ms\n",
      "Speed: 2.4ms preprocess, 278.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 31 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [387.9268, 270.4185],\n",
      "         [309.4799, 298.1533],\n",
      "         [387.1865, 319.5172],\n",
      "         [264.8009, 350.6582],\n",
      "         [386.8123, 386.0949],\n",
      "         [252.4741, 402.9442],\n",
      "         [366.0081, 409.4512],\n",
      "         [276.0278, 418.9955],\n",
      "         [316.5699, 426.8118],\n",
      "         [313.2973, 500.9523],\n",
      "         [281.0911, 520.2363],\n",
      "         [318.7632, 592.8123],\n",
      "         [218.4442, 617.0469]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 198.4ms\n",
      "Speed: 1.5ms preprocess, 198.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 32 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [340.9850, 257.7096],\n",
      "         [379.6459, 266.9733],\n",
      "         [305.1213, 296.0837],\n",
      "         [381.9319, 316.8736],\n",
      "         [264.4098, 353.5028],\n",
      "         [382.0037, 388.6843],\n",
      "         [248.2333, 405.8040],\n",
      "         [358.7789, 416.0386],\n",
      "         [275.7222, 417.2120],\n",
      "         [316.0267, 425.1973],\n",
      "         [306.2274, 498.3426],\n",
      "         [275.3437, 520.3567],\n",
      "         [318.3041, 591.0990],\n",
      "         [217.0576, 619.1444]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 200.1ms\n",
      "Speed: 2.0ms preprocess, 200.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 33 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [377.0268, 266.4894],\n",
      "         [308.1497, 292.1817],\n",
      "         [372.5817, 315.0917],\n",
      "         [258.4174, 347.1581],\n",
      "         [370.3787, 385.5986],\n",
      "         [252.6393, 402.2853],\n",
      "         [355.0977, 402.6431],\n",
      "         [280.5159, 421.3561],\n",
      "         [311.5246, 427.9304],\n",
      "         [307.8984, 503.1567],\n",
      "         [268.5612, 519.9365],\n",
      "         [311.9036, 595.2906],\n",
      "         [210.4690, 620.7665]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 189.2ms\n",
      "Speed: 3.2ms preprocess, 189.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 34 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [375.1492, 259.1491],\n",
      "         [303.7860, 288.7973],\n",
      "         [371.9495, 310.3351],\n",
      "         [253.7939, 348.9889],\n",
      "         [363.2125, 383.1597],\n",
      "         [251.1884, 398.4273],\n",
      "         [345.2533, 392.3856],\n",
      "         [277.4171, 414.1553],\n",
      "         [312.0403, 420.0633],\n",
      "         [305.0718, 501.6658],\n",
      "         [273.7139, 516.7020],\n",
      "         [306.5105, 593.5218],\n",
      "         [216.1403, 617.0478]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 211.3ms\n",
      "Speed: 1.4ms preprocess, 211.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 35 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [370.9713, 254.1356],\n",
      "         [301.9176, 285.2281],\n",
      "         [368.2516, 304.1791],\n",
      "         [255.7814, 348.1766],\n",
      "         [362.4901, 371.2065],\n",
      "         [248.0195, 404.5336],\n",
      "         [347.8470, 386.5254],\n",
      "         [278.2837, 409.2224],\n",
      "         [312.3917, 414.5553],\n",
      "         [301.1305, 501.6281],\n",
      "         [277.5703, 517.2790],\n",
      "         [299.6644, 595.1449],\n",
      "         [216.9734, 617.9086]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 4 persons, 185.2ms\n",
      "Speed: 1.5ms preprocess, 185.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 36 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [366.6754, 250.6003],\n",
      "         [294.6962, 279.2702],\n",
      "         [368.3300, 298.3276],\n",
      "         [250.1824, 338.9423],\n",
      "         [362.8179, 366.5099],\n",
      "         [252.1545, 398.8868],\n",
      "         [347.6067, 396.7033],\n",
      "         [277.4737, 403.9656],\n",
      "         [317.2935, 408.8734],\n",
      "         [298.8162, 499.5596],\n",
      "         [282.4387, 513.0251],\n",
      "         [291.7295, 595.3828],\n",
      "         [219.3603, 615.1791]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 187.3ms\n",
      "Speed: 1.2ms preprocess, 187.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 37 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [365.8011, 247.3016],\n",
      "         [289.2189, 277.0402],\n",
      "         [371.5350, 297.7696],\n",
      "         [243.8289, 334.8773],\n",
      "         [362.7679, 364.7433],\n",
      "         [251.2111, 394.0995],\n",
      "         [345.4273, 396.0599],\n",
      "         [277.5478, 401.7320],\n",
      "         [324.0977, 406.8474],\n",
      "         [294.8953, 500.2872],\n",
      "         [288.1773, 512.4119],\n",
      "         [283.9791, 595.7750],\n",
      "         [221.8172, 610.9446]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 4 persons, 197.7ms\n",
      "Speed: 1.6ms preprocess, 197.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 38 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [324.7841, 238.0181],\n",
      "         [364.2849, 246.2939],\n",
      "         [286.1985, 277.2984],\n",
      "         [369.6840, 297.6865],\n",
      "         [238.7974, 334.3299],\n",
      "         [360.8254, 366.4690],\n",
      "         [248.4161, 396.0670],\n",
      "         [344.2488, 404.3602],\n",
      "         [273.8739, 402.6315],\n",
      "         [322.0581, 407.5299],\n",
      "         [291.4785, 500.8657],\n",
      "         [292.7046, 511.8521],\n",
      "         [277.3935, 595.0989],\n",
      "         [230.9827, 609.1392]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 4 persons, 187.4ms\n",
      "Speed: 1.3ms preprocess, 187.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 39 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [360.7201, 247.9672],\n",
      "         [284.7462, 277.7233],\n",
      "         [366.3185, 299.6749],\n",
      "         [238.5604, 332.3417],\n",
      "         [358.6299, 367.8794],\n",
      "         [252.2696, 395.0946],\n",
      "         [346.6968, 410.6082],\n",
      "         [274.5161, 404.5102],\n",
      "         [321.9422, 410.1766],\n",
      "         [297.1395, 503.6131],\n",
      "         [300.4692, 514.3055],\n",
      "         [279.5180, 593.9509],\n",
      "         [237.1184, 607.4154]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 188.4ms\n",
      "Speed: 1.2ms preprocess, 188.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 40 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [357.5709, 247.5730],\n",
      "         [285.3347, 283.7004],\n",
      "         [361.7089, 299.1259],\n",
      "         [246.4158, 344.5166],\n",
      "         [359.9815, 370.1825],\n",
      "         [256.5691, 403.5385],\n",
      "         [351.2466, 416.2353],\n",
      "         [275.4749, 413.5722],\n",
      "         [320.7279, 416.9991],\n",
      "         [305.5342, 507.4338],\n",
      "         [314.8135, 514.0111],\n",
      "         [277.3873, 592.4254],\n",
      "         [246.3196, 601.6070]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 185.6ms\n",
      "Speed: 1.3ms preprocess, 185.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 41 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [355.5399, 247.3544],\n",
      "         [287.4906, 283.5110],\n",
      "         [357.5833, 299.3211],\n",
      "         [253.8293, 346.1805],\n",
      "         [359.6273, 369.9693],\n",
      "         [261.0081, 409.8516],\n",
      "         [354.7041, 417.4686],\n",
      "         [276.9514, 414.7605],\n",
      "         [319.0261, 419.2924],\n",
      "         [311.4843, 504.5288],\n",
      "         [325.5229, 515.0287],\n",
      "         [276.4672, 585.1407],\n",
      "         [256.0186, 600.9138]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 4 persons, 188.0ms\n",
      "Speed: 1.8ms preprocess, 188.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 42 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [357.2251, 245.7123],\n",
      "         [287.5508, 283.7824],\n",
      "         [362.2707, 295.2039],\n",
      "         [263.2966, 346.7481],\n",
      "         [366.9983, 367.8867],\n",
      "         [271.9581, 393.4312],\n",
      "         [359.9878, 419.5516],\n",
      "         [279.5190, 414.4856],\n",
      "         [326.2622, 417.3340],\n",
      "         [309.5876, 505.8395],\n",
      "         [336.1884, 512.6235],\n",
      "         [277.4528, 584.8788],\n",
      "         [279.3371, 596.6074]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 184.3ms\n",
      "Speed: 1.2ms preprocess, 184.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 43 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [356.4751, 243.9865],\n",
      "         [288.7627, 284.8018],\n",
      "         [362.8802, 290.8336],\n",
      "         [267.1728, 349.9237],\n",
      "         [370.1999, 362.9192],\n",
      "         [276.5724, 389.3308],\n",
      "         [363.9976, 414.2735],\n",
      "         [284.0246, 413.7140],\n",
      "         [332.5657, 413.9529],\n",
      "         [306.0835, 507.9676],\n",
      "         [346.9235, 506.2488],\n",
      "         [277.4211, 593.4462],\n",
      "         [303.7924, 592.0049]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 183.8ms\n",
      "Speed: 1.4ms preprocess, 183.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 44 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [360.2438, 242.0806],\n",
      "         [292.6696, 285.2873],\n",
      "         [369.9706, 286.9943],\n",
      "         [275.9953, 348.4060],\n",
      "         [375.9737, 358.1968],\n",
      "         [285.8860, 379.2467],\n",
      "         [367.8321, 405.3141],\n",
      "         [291.5494, 409.7722],\n",
      "         [342.9746, 408.3325],\n",
      "         [305.3068, 508.1733],\n",
      "         [351.1997, 496.6642],\n",
      "         [282.7654, 601.1403],\n",
      "         [311.8970, 578.8029]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 186.3ms\n",
      "Speed: 1.4ms preprocess, 186.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 45 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [362.5244, 239.6223],\n",
      "         [293.6949, 284.5511],\n",
      "         [372.8470, 283.2433],\n",
      "         [279.6343, 349.1797],\n",
      "         [378.7133, 354.9278],\n",
      "         [289.1235, 367.1210],\n",
      "         [371.9152, 399.8428],\n",
      "         [294.5294, 406.9653],\n",
      "         [348.9772, 405.0834],\n",
      "         [306.5936, 504.7647],\n",
      "         [367.5978, 494.5398],\n",
      "         [279.1003, 597.2314],\n",
      "         [331.0755, 579.0312]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 190.4ms\n",
      "Speed: 1.3ms preprocess, 190.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 46 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [366.5152, 236.9266],\n",
      "         [300.5834, 282.4692],\n",
      "         [375.1354, 281.1561],\n",
      "         [286.2828, 346.7491],\n",
      "         [379.2491, 348.8502],\n",
      "         [295.0859, 365.1907],\n",
      "         [376.0899, 386.6818],\n",
      "         [298.8527, 409.0672],\n",
      "         [352.8845, 406.6138],\n",
      "         [305.5715, 504.2361],\n",
      "         [378.8339, 496.0236],\n",
      "         [274.0932, 599.3372],\n",
      "         [353.1465, 588.5977]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 187.9ms\n",
      "Speed: 1.5ms preprocess, 187.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 47 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [371.5905, 236.4125],\n",
      "         [307.6678, 281.3309],\n",
      "         [381.8655, 283.5952],\n",
      "         [291.1114, 341.9942],\n",
      "         [386.3241, 353.2910],\n",
      "         [297.8155, 360.6818],\n",
      "         [382.8568, 385.6301],\n",
      "         [305.6338, 407.0776],\n",
      "         [360.3733, 406.3294],\n",
      "         [304.9350, 504.1124],\n",
      "         [385.2588, 499.1258],\n",
      "         [274.8297, 601.7650],\n",
      "         [371.9059, 595.9126]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 184.1ms\n",
      "Speed: 1.2ms preprocess, 184.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 48 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [375.4319, 236.3049],\n",
      "         [310.2512, 283.2151],\n",
      "         [388.6374, 285.5009],\n",
      "         [297.6714, 347.1659],\n",
      "         [398.6434, 357.4958],\n",
      "         [305.4057, 362.4036],\n",
      "         [389.3432, 385.8029],\n",
      "         [311.6140, 405.6903],\n",
      "         [369.9882, 406.1040],\n",
      "         [304.6723, 504.5226],\n",
      "         [394.2385, 505.0300],\n",
      "         [276.7889, 602.5352],\n",
      "         [393.8547, 606.3008]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 4 persons, 329.4ms\n",
      "Speed: 1.2ms preprocess, 329.4ms inference, 8.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 49 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [382.2216, 239.3201],\n",
      "         [316.2450, 286.6963],\n",
      "         [397.6490, 289.4525],\n",
      "         [303.2943, 355.2264],\n",
      "         [408.3327, 363.4432],\n",
      "         [323.6964, 367.1572],\n",
      "         [393.7992, 385.0799],\n",
      "         [321.4544, 409.9749],\n",
      "         [381.8924, 409.3391],\n",
      "         [305.5100, 508.3035],\n",
      "         [398.5887, 507.8722],\n",
      "         [279.1177, 605.3804],\n",
      "         [403.1375, 610.5330]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 223.2ms\n",
      "Speed: 27.2ms preprocess, 223.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 50 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [385.8896, 241.5902],\n",
      "         [318.5792, 286.9080],\n",
      "         [404.0536, 291.1876],\n",
      "         [309.6569, 355.5630],\n",
      "         [417.0229, 365.0471],\n",
      "         [338.6961, 373.4738],\n",
      "         [399.4454, 387.9908],\n",
      "         [326.8557, 410.3860],\n",
      "         [389.8384, 410.6186],\n",
      "         [311.6175, 508.1116],\n",
      "         [400.2484, 510.3054],\n",
      "         [282.8013, 603.9081],\n",
      "         [401.9015, 613.2856]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 4 persons, 206.7ms\n",
      "Speed: 1.3ms preprocess, 206.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 51 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [390.8613, 239.5162],\n",
      "         [321.7008, 287.9201],\n",
      "         [412.5023, 287.5534],\n",
      "         [317.3351, 357.6423],\n",
      "         [429.9297, 363.8241],\n",
      "         [335.9799, 368.4180],\n",
      "         [410.9372, 390.6522],\n",
      "         [329.3822, 410.5933],\n",
      "         [395.0926, 410.5549],\n",
      "         [317.6501, 508.7718],\n",
      "         [402.0670, 512.6833],\n",
      "         [289.1879, 604.2771],\n",
      "         [397.4150, 614.0795]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 185.9ms\n",
      "Speed: 1.1ms preprocess, 185.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 52 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [397.7150, 237.0823],\n",
      "         [331.1967, 286.1335],\n",
      "         [416.8564, 286.2580],\n",
      "         [321.0513, 354.7462],\n",
      "         [435.9792, 360.0264],\n",
      "         [336.5098, 367.0126],\n",
      "         [413.9343, 385.3935],\n",
      "         [336.3339, 411.7995],\n",
      "         [397.1326, 410.5105],\n",
      "         [325.5508, 508.7365],\n",
      "         [402.2129, 510.5713],\n",
      "         [301.4832, 602.4168],\n",
      "         [392.8863, 612.2952]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 192.7ms\n",
      "Speed: 1.3ms preprocess, 192.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 53 keypoints shape: torch.Size([1, 17, 2])\n",
      "Keypoints data: tensor([[[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [399.9077, 233.6383],\n",
      "         [336.5876, 281.6636],\n",
      "         [419.2255, 281.6190],\n",
      "         [324.0275, 348.2240],\n",
      "         [437.7485, 354.7332],\n",
      "         [336.2057, 366.5742],\n",
      "         [418.4501, 386.2067],\n",
      "         [343.4452, 407.1776],\n",
      "         [400.6390, 405.6442],\n",
      "         [333.8528, 508.1288],\n",
      "         [399.8238, 508.8639],\n",
      "         [314.4912, 606.0416],\n",
      "         [389.6958, 614.0881]]])\n",
      "Processed 54 frames\n",
      "Keypoints data saved to keypoints_data.npy\n",
      "Keypoints data shape: (54, 17, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# モデルの読み込み。姿勢推論用のモデルデータを読み込む\n",
    "model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# 入力ビデオファイルのパス\n",
    "input_video = \"/Users/yusuke.s/Documents/pickleball_videos/forehand_volley/xfhjsg.mp4\"\n",
    "\n",
    "# 出力ビデオファイルのパス\n",
    "output_video = \"output_video.mp4\"\n",
    "\n",
    "# ビデオキャプチャオブジェクトの作成\n",
    "cap = cv2.VideoCapture(input_video)\n",
    "\n",
    "# ビデオの属性を取得\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# 出力ビデオライターの設定\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
    "\n",
    "# 座標データを保存するリスト\n",
    "all_keypoints = []\n",
    "\n",
    "frame_count = 0\n",
    "while cap.isOpened():\n",
    "    # フレームを読み込む\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # YOLOモデルで推論を行う\n",
    "    results = model(frame)\n",
    "\n",
    "    # 結果を描画したフレームを取得\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    # キーポイントの座標を取得\n",
    "    if len(results[0].boxes) > 0:\n",
    "        # バウンディングボックスの面積を計算\n",
    "        areas = [(box.xyxy[0][2] - box.xyxy[0][0]) * (box.xyxy[0][3] - box.xyxy[0][1]) for box in results[0].boxes]\n",
    "        # 最大面積を持つバウンディングボックスのインデックスを取得\n",
    "        largest_box_index = np.argmax(areas)\n",
    "        \n",
    "        keypoints = results[0].keypoints[largest_box_index]  # 最大の長方形に対応するキーポイント\n",
    "        \n",
    "        # デバッグ出力\n",
    "        print(f\"Frame {frame_count} keypoints shape: {keypoints.xy.shape}\")\n",
    "        print(f\"Keypoints data: {keypoints.xy}\")\n",
    "\n",
    "        # キーポイントが2次元配列であり、少なくとも2つの値を持つことを確認\n",
    "        if keypoints.xy.ndim == 3 and keypoints.xy.shape[2] >= 2:\n",
    "            all_keypoints.append(keypoints.xy[0].cpu().numpy())  # 最大の長方形の人物のキーポイントのみを保存\n",
    "\n",
    "            # キーポイントを描画（オプション）\n",
    "            for point in keypoints.xy[0]:\n",
    "                x, y = point[:2]\n",
    "                cv2.circle(annotated_frame, (int(x), int(y)), 5, (0, 255, 0), -1)\n",
    "        else:\n",
    "            print(f\"Warning: Invalid keypoint data in frame {frame_count}\")\n",
    "\n",
    "    # 処理したフレームを出力ビデオに書き込む\n",
    "    out.write(annotated_frame)\n",
    "\n",
    "    # 処理状況を表示（オプション）\n",
    "    cv2.imshow(\"YOLOv8 Inference\", annotated_frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "# リソースの解放\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# 座標データをNumPy配列として保存\n",
    "if all_keypoints:\n",
    "    np.save('keypoints_data.npy', np.array(all_keypoints))\n",
    "    print(f\"Processed {frame_count} frames\")\n",
    "    print(f\"Keypoints data saved to keypoints_data.npy\")\n",
    "    \n",
    "    # キーポイントデータの形状を確認\n",
    "    keypoints_array = np.array(all_keypoints)\n",
    "    print(f\"Keypoints data shape: {keypoints_array.shape}\")\n",
    "else:\n",
    "    print(\"No valid keypoints data was collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a234313a-13d6-486b-9747-0e7600866b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
