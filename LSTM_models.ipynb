{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yusuke-satani/swing_classification/blob/main/LSTM_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05bd3efc-8622-4368-a0c0-12916549c093",
      "metadata": {
        "id": "05bd3efc-8622-4368-a0c0-12916549c093",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdf97f34-b5e9-4bde-8439-4fe79b95e509",
      "metadata": {
        "id": "bdf97f34-b5e9-4bde-8439-4fe79b95e509"
      },
      "outputs": [],
      "source": [
        "model = YOLO('yolov8n-pose.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaf9639c-d106-4ab0-b683-9f414ca254a9",
      "metadata": {
        "scrolled": true,
        "id": "aaf9639c-d106-4ab0-b683-9f414ca254a9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def extract_keypoints(frames):\n",
        "    all_keypoints = []\n",
        "    device = torch.device('cuda')\n",
        "    for frame in frames:\n",
        "        frame = torch.tensor(frame, dtype=torch.float32).to(device)\n",
        "        results = model(frame)\n",
        "\n",
        "        if len(results) > 0 and len(results[0].keypoints) > 0:\n",
        "            keypoints_list = results[0].keypoints\n",
        "            bboxes = results[0].boxes  # list of bounding boes\n",
        "\n",
        "            if bboxes is not None and len(bboxes) > 0:\n",
        "                # calculate area of bounding boxes\n",
        "                try:\n",
        "                    xyxy = bboxes.xyxy.cpu().numpy()  # get cordinates of bounding boxes\n",
        "                    areas = [(box[2] - box[0]) * (box[3] - box[1]) for box in xyxy]\n",
        "                except IndexError as e:\n",
        "                    print(f\"Error calculating areas: {e}\")\n",
        "                    print(f\"Bounding boxes: {bboxes}\")\n",
        "                    continue\n",
        "\n",
        "                # get the index of the biggest bounding box\n",
        "                max_area_index = np.argmax(areas)\n",
        "\n",
        "                # get keypoints of the biggest bounding box\n",
        "                keypoints = keypoints_list[max_area_index].xy[0].cpu().numpy()\n",
        "\n",
        "                all_keypoints.append(keypoints)\n",
        "            else:\n",
        "                print(f\"Warning: No bounding boxes found in frame.\")\n",
        "        else:\n",
        "            print(f\"Warning: No keypoints found in frame.\")\n",
        "\n",
        "    return all_keypoints  # np.arrayの変換を削除\n",
        "\n",
        "\n",
        "def normalize_keypoints(keypoints):\n",
        "    hip_index = 11  #  index of left hip\n",
        "    shoulder_index = 5  # index of shoulder\n",
        "\n",
        "    normalized_keypoints = []\n",
        "    for frame_keypoints in keypoints:\n",
        "        if len(frame_keypoints) > max(hip_index, shoulder_index):\n",
        "            hip_point = frame_keypoints[hip_index]\n",
        "            shoulder_point = frame_keypoints[shoulder_index]\n",
        "\n",
        "            # Calculate the point at the waist as the origin (0,0) and the other points as relative positions to make the model robust\n",
        "            relative_points = frame_keypoints - hip_point\n",
        "\n",
        "            scale_factor = np.linalg.norm(shoulder_point - hip_point)\n",
        "            if scale_factor != 0:\n",
        "                relative_points /= scale_factor\n",
        "\n",
        "            normalized_keypoints.append(relative_points)\n",
        "        else:\n",
        "            print(f\"Warning: Frame with insufficient keypoints detected. Skipping this frame.\")\n",
        "\n",
        "    return np.array(normalized_keypoints)\n",
        "\n",
        "def process_folder(folder_path, label):\n",
        "    print(f\"Processing folder: {folder_path}\")\n",
        "    video_data = []\n",
        "    for video_file in os.listdir(folder_path):\n",
        "        if video_file.endswith(('.mp4', '.avi', '.mov')):\n",
        "            video_path = os.path.join(folder_path, video_file)\n",
        "            frames = process_video(video_path)\n",
        "            keypoints = extract_keypoints(frames)\n",
        "            if len(keypoints) > 0:\n",
        "                normalized_keypoints = normalize_keypoints(keypoints)\n",
        "                if len(normalized_keypoints) > 0:\n",
        "                    video_data.append((normalized_keypoints, label))\n",
        "                else:\n",
        "                    print(f\"Warning: No valid keypoints found in video {video_file}\")\n",
        "            else:\n",
        "                print(f\"Warning: No keypoints detected in video {video_file}\")\n",
        "    return video_data\n",
        "\n",
        "    print(f\"Number of videos processed: {len(video_data)}\")\n",
        "\n",
        "shot_types = ['forehand_stroke','forehand_slice','forehand_volley', 'backhand_stroke', 'backhand_volley', 'backhand_slice']\n",
        "all_data = []\n",
        "for label, shot_type in enumerate(shot_types):\n",
        "    folder_path = f'/Users/yusuke.s/Documents/pickleball_videos/{shot_type}'\n",
        "    all_data.extend(process_folder(folder_path, label))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qkfomhpzIXs5"
      },
      "id": "qkfomhpzIXs5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04e037d6-7abd-4646-bb7e-429754216e95",
      "metadata": {
        "id": "04e037d6-7abd-4646-bb7e-429754216e95"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "all_data = np.load('/content/drive/MyDrive/all_data.npy', allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "335839cf-fa3d-4fcb-b693-fa2bb529abea",
      "metadata": {
        "id": "335839cf-fa3d-4fcb-b693-fa2bb529abea"
      },
      "outputs": [],
      "source": [
        "def pad_sequences(sequences, max_length):\n",
        "    # Align sequence lengths\n",
        "    return [seq[:max_length] if len(seq) > max_length else np.pad(seq, ((0, max_length - len(seq)), (0, 0), (0, 0)), 'constant') for seq in sequences]\n",
        "\n",
        "X = [data[0] for data in all_data]\n",
        "y = [data[1] for data in all_data]\n",
        "\n",
        "# Align sequence lengths\n",
        "X_padded = pad_sequences(X, 100)\n",
        "\n",
        "X_array = np.array(X_padded)\n",
        "y_array = np.array(y)\n",
        "shot_types = ['forehand_stroke','forehand_volley','forehand_slice','backhand_stroke','backhand_volley','backhand_slice']\n",
        "y_onehot = np.eye(len(shot_types))[y_array]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c57d974-909f-4cae-a548-173e44fb2cae",
      "metadata": {
        "id": "2c57d974-909f-4cae-a548-173e44fb2cae"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, AdditiveAttention, Concatenate\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, x_set, y_set, batch_size):\n",
        "        self.x, self.y = x_set, y_set\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        return batch_x, batch_y\n",
        "\n",
        "def create_lstm_attention_model(sequence_length, num_keypoints, num_coords, num_classes):\n",
        "    inputs = Input(shape=(sequence_length, num_keypoints * num_coords))\n",
        "    x = LSTM(128, return_sequences=True)(inputs)\n",
        "    attention = AdditiveAttention()([x, x])\n",
        "    x = Concatenate()([x, attention])\n",
        "    x = LSTM(64, return_sequences=True)(x)\n",
        "    attention = AdditiveAttention()([x, x])\n",
        "    x = Concatenate()([x, attention])\n",
        "    x = LSTM(32)(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_array, y_onehot, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], -1)\n",
        "X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], -1)\n",
        "\n",
        "train_generator = DataGenerator(X_train, y_train, batch_size=16)\n",
        "val_generator = DataGenerator(X_val, y_val, batch_size=16)\n",
        "test_generator = DataGenerator(X_test, y_test, batch_size=16)\n",
        "\n",
        "sequence_length = X_train.shape[1]\n",
        "num_keypoints = 17\n",
        "num_coords = 2\n",
        "num_classes = 6\n",
        "model = create_lstm_attention_model(sequence_length, num_keypoints, num_coords, num_classes)\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(np.argmax(y_train, axis=1)), y=np.argmax(y_train, axis=1))\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=100,\n",
        "    callbacks=[early_stopping],\n",
        "    class_weight=class_weight_dict\n",
        ")\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(f\"Test accuracy: {test_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d22da0e1-3977-4aa5-bcf8-062349740cda",
      "metadata": {
        "id": "d22da0e1-3977-4aa5-bcf8-062349740cda"
      },
      "outputs": [],
      "source": [
        "model.save('swing_class.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e850997-b4f9-452b-892c-4c69f63232d7",
      "metadata": {
        "id": "6e850997-b4f9-452b-892c-4c69f63232d7"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "loaded_model = load_model('/content/drive/MyDrive/swing_class.h5')\n",
        "\n",
        "predictions = loaded_model.predict(test_generator)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator\n",
        "np.argmax(predictions, axis=1)"
      ],
      "metadata": {
        "id": "e5dT6YE1YPz5"
      },
      "id": "e5dT6YE1YPz5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hu1wkiDYX7Kw"
      },
      "id": "Hu1wkiDYX7Kw",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}